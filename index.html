<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>VTC-Bench: Are We Using the Right Benchmark?</title>
    
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600&family=Playfair+Display:wght@700&display=swap" rel="stylesheet">
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <style>
        :root {
            --primary-color: #2c3e50;
            --accent-color: #3498db;
            --bg-color: #ffffff;
            --text-color: #4a4a4a;
            --light-bg: #f8f9fa;
        }

        body {
            font-family: 'Inter', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            background-color: var(--bg-color);
        }

        h1, h2, h3 {
            font-family: 'Playfair Display', serif;
            color: var(--primary-color);
            margin-bottom: 0.5em;
        }

        h1 { font-size: 2.5rem; line-height: 1.2; text-align: center; margin-bottom: 20px; }
        h2 { font-size: 1.8rem; border-bottom: 2px solid var(--light-bg); padding-bottom: 10px; margin-top: 50px; }
        h3 { font-size: 1.3rem; margin-top: 25px; }

        a { color: var(--accent-color); text-decoration: none; transition: 0.3s; }
        a:hover { color: #1d6fa5; }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 40px 20px;
        }

        /* Header Section */
        .header { text-align: center; margin-bottom: 50px; }
        
        .authors { font-size: 1.1rem; margin-bottom: 10px; }
        .author-block { display: inline-block; margin-right: 15px; }
        .affiliations { font-size: 0.9rem; color: #7f8c8d; margin-top: 10px; margin-bottom: 20px; }
        .notes { font-size: 0.85rem; color: #95a5a6; font-style: italic; }

        /* Buttons */
        .link-buttons {
            display: flex;
            justify-content: center;
            gap: 15px;
            margin-top: 20px;
            flex-wrap: wrap;
        }
        .btn {
            background-color: var(--primary-color);
            color: white;
            padding: 10px 20px;
            border-radius: 30px;
            font-weight: 600;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }
        .btn:hover {
            background-color: #1a252f;
            transform: translateY(-2px);
            color: white;
        }

        /* Abstract */
        .abstract-box {
            background-color: var(--light-bg);
            padding: 30px;
            border-radius: 12px;
            margin: 30px 0;
            border-left: 5px solid var(--primary-color);
        }
        .abstract-title { font-weight: bold; text-transform: uppercase; letter-spacing: 1px; margin-bottom: 10px; display: block; color: var(--primary-color); }

        /* Images */
        .figure {
            width: 100%;
            margin: 30px 0;
            text-align: center;
        }
        .figure img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 5px 15px rgba(0,0,0,0.05);
            margin-bottom: 10px;
        }
        .caption {
            font-size: 0.9rem;
            color: #666;
            margin-top: 8px;
            text-align: justify;
        }

        /* Tables */
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            font-size: 0.9rem;
        }
        th, td {
            padding: 12px;
            text-align: center;
            border-bottom: 1px solid #ddd;
        }
        th { background-color: var(--light-bg); font-weight: 600; }
        tr:hover { background-color: #f1f1f1; }
        .highlight-row { background-color: #e8f6f3; font-weight: bold; }

        /* Code Block */
        pre {
            background-color: #f4f4f4;
            padding: 20px;
            border-radius: 8px;
            overflow-x: auto;
            font-size: 0.85rem;
            line-height: 1.4;
        }

        /* Responsive */
        @media (max-width: 600px) {
            h1 { font-size: 1.8rem; }
            .btn { width: 100%; justify-content: center; }
        }
    </style>
</head>
<body>

<div class="container">
    
    <header class="header">
        <h1>Are We Using the Right Benchmark?<br>An Evaluation Framework for Visual Token Compression Methods</h1>
        
        <div class="authors">
            <span class="author-block">Chenfei Liao<sup>1,2,6</sup></span>
            <span class="author-block">Wensong Wang<sup>3,2</sup></span>
            <span class="author-block">Zichen Wen<sup>2,5</sup></span>
            <span class="author-block">Xu Zheng<sup>1,4,6</sup></span>
            <span class="author-block">Yiyu Wang<sup>2</sup></span>
            <br>
            <span class="author-block">Haocong He<sup>2</sup></span>
            <span class="author-block">Yuanhuiyi Lyu<sup>1,6</sup></span>
            <span class="author-block">Lutao Jiang<sup>1,6</sup></span>
            <span class="author-block">Xin Zou<sup>1,6</sup></span>
            <span class="author-block">Yuqian Fu<sup>4</sup></span>
            <span class="author-block">Bin Ren<sup>7,8,4</sup></span>
            <br>
            <span class="author-block">Linfeng Zhang<sup>†</sup></span>
            <span class="author-block">Xuming Hu<sup>1,6,†</sup></span>
        </div>

        <div class="affiliations">
            <sup>1</sup>HKUST (Guangzhou) &nbsp;
            <sup>2</sup>Shanghai AI Laboratory &nbsp;
            <sup>3</sup>Northeastern University &nbsp;
            <sup>4</sup>HKUST<br>
            <sup>5</sup>Shanghai Jiao Tong University &nbsp;
            <sup>6</sup>INSAIT &nbsp;
            <sup>7</sup>University of Pisa &nbsp;
            <sup>8</sup>University of Trento
        </div>

        <div class="notes">
            <sup>†</sup> Corresponding Authors
        </div>

        <div class="link-buttons">
            <a href="https://arxiv.org/abs/2510.07143" class="btn" target="_blank"><i class="fas fa-file-pdf"></i> Paper (arXiv)</a>
            <a href="https://github.com/Chenfei-Liao/VTC-Bench" class="btn" target="_blank"><i class="fab fa-github"></i> Code & Data</a>
        </div>
    </header>

    <div class="figure">
        <img src="Figure2.svg" alt="Average Decline Ratio and inference time">
        <div class="caption">
            <strong>Figure 2.</strong> (a) Average Decline Ratio (ADR) of five visual token compression methods on eight benchmarks (Qwen2-VL-7B). (b) Inference time per image comparison of DART and Downsample on MMStar at 0.75 compression (1× A800).
        </div>
    </div>

    <div class="abstract-box">
        <span class="abstract-title">Abstract</span>
        <p>
            Multimodal LLMs rely on visual token compression to speed up inference. Yet mainstream benchmarks were built for perception/reasoning, not for testing fidelity under compression. We find that <strong>simple image downsampling</strong> often matches or exceeds state-of-the-art compression methods, implying many samples are insensitive to fine-grained visual details.
        </p>
        <p>
            We introduce <strong>VTC-Bench</strong>, a framework that uses downsampling as a discriminator to split data into <strong>Group B (Simple)</strong>—samples still correct after aggressive downsampling—and <strong>Group A (Difficult)</strong>—samples that fail after downsampling. Evaluating compression on Group A yields a difficulty-aware, fairer comparison.
        </p>
        <p>
            On Group A, advanced methods show clear gains over naive downsampling; on Group B the gains vanish. VTC-Bench thus exposes the true strengths of visual token compression methods.
        </p>
    </div>

    <section>
        <h2>1. Motivation: Simplicity Bias in Current Benchmarks</h2>
        <p>
            Many benchmarks can be solved with low-resolution cues, masking the benefits of sophisticated compression. Downsampling alone can retain accuracy while reducing tokens, exposing the simplicity bias.
        </p>
    </section>

    <section>
        <h2>2. VTC-Bench Framework</h2>
        <p>
            We filter existing benchmarks (e.g., GQA, MMBench, ChartQA, MME) by comparing original vs. downsampled performance:
        </p>

        <div class="figure">
            <img src="Figure1.svg" alt="VTC-Bench framework">
            <div class="caption">
                <strong>Figure 1.</strong> VTC-Bench framework: starting from samples answered correctly at original resolution, downsampling filters the data into difficult (Group A) and simple (Group B) subsets for fair evaluation of visual token compression.
            </div>
        </div>

        <h3>Grouping Logic</h3>
        <ul>
            <li><strong>Group B (Simple):</strong> Origin correct & downsample correct.</li>
            <li><strong>Group A (Difficult):</strong> Origin correct & downsample wrong.</li>
        </ul>
        <p>We focus evaluation on Group A to measure detail preservation and robustness under compression.</p>
    </section>

    <section>
        <h2>3. Results & Insights</h2>
        <div class="figure">
            <img src="Figure4.svg" alt="Group A vs Group B comparison at 75% compression">
            <div class="caption">
                <strong>Figure 4.</strong> Comparison of advanced token compression methods and downsampling on Qwen2-VL-7B by groups at 75% compression (Group A: difficult; Group B: simple).
            </div>
        </div>
        <div class="figure">
            <img src="Figure3.svg" alt="VTC-Bench results on Qwen2-VL-7B">
            <div class="caption">
                <strong>Figure 3.</strong> VTC-Bench results on Qwen2-VL-7B after filtering for difficult samples (Group A).
            </div>
        </div>
        <h3>Major Takeaways</h3>
        <ul>
            <li><strong>显著组间差距：</strong> 简单样本（Group B）精度远高于困难样本（Group A），基准中混有大量易样本。</li>
            <li><strong>下采样提供参考点：</strong> Group B 中下采样可达 100%，Group A 为 0%；高级方法在 Group A 才展现优势。</li>
            <li><strong>公平评测：</strong> 聚焦 Group A 放大方法差异，凸显复杂压缩在细粒度任务（OCR、ChartQA）上的价值。</li>
        </ul>
    </section>

    <section>
        <h2>Citation</h2>
        <p>If you find VTC-Bench useful for your research, please cite our paper:</p>
        <pre><code>@article{liao2025are,
  title={Are We Using the Right Benchmark: An Evaluation Framework for Visual Token Compression Methods},
  author={Liao, Chenfei and Wang, Wensong and Wen, Zichen and Zheng, Xu and Wang, Yiyu and He, Haocong and Lyu, Yuanhuiyi and Jiang, Lutao and Zou, Xin and Fu, Yuqian and Ren, Bin and Zhang, Linfeng and Hu, Xuming},
  journal={arXiv preprint arXiv:2510.07143},
  year={2025}
}</code></pre>
    </section>

    <footer style="text-align: center; margin-top: 50px; font-size: 0.8rem; color: #888;">
        <p>&copy; 2025 VTC-Bench Project. Based on arXiv:2510.07143.</p>
    </footer>

</div>

</body>
</html>
