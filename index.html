<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>VTC-Bench: Are We Using the Right Benchmark?</title>
    
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600&family=Playfair+Display:wght@700&display=swap" rel="stylesheet">
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <style>
        :root {
            --primary-color: #2c3e50;
            --accent-color: #3498db;
            --bg-color: #ffffff;
            --text-color: #4a4a4a;
            --light-bg: #f8f9fa;
        }

        body {
            font-family: 'Inter', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            background-color: var(--bg-color);
        }

        h1, h2, h3 {
            font-family: 'Playfair Display', serif;
            color: var(--primary-color);
            margin-bottom: 0.5em;
        }

        h1 { font-size: 2.5rem; line-height: 1.2; text-align: center; margin-bottom: 20px; }
        h2 { font-size: 1.8rem; border-bottom: 2px solid var(--light-bg); padding-bottom: 10px; margin-top: 50px; }
        h3 { font-size: 1.3rem; margin-top: 25px; }

        a { color: var(--accent-color); text-decoration: none; transition: 0.3s; }
        a:hover { color: #1d6fa5; }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 40px 20px;
        }

        /* Header Section */
        .header { text-align: center; margin-bottom: 50px; }
        
        .authors { font-size: 1.1rem; margin-bottom: 10px; }
        .author-block { display: inline-block; margin-right: 15px; }
        .affiliations { font-size: 0.9rem; color: #7f8c8d; margin-top: 10px; margin-bottom: 20px; }
        .notes { font-size: 0.85rem; color: #95a5a6; font-style: italic; }

        /* Buttons */
        .link-buttons {
            display: flex;
            justify-content: center;
            gap: 15px;
            margin-top: 20px;
            flex-wrap: wrap;
        }
        .btn {
            background-color: var(--primary-color);
            color: white;
            padding: 10px 20px;
            border-radius: 30px;
            font-weight: 600;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }
        .btn:hover {
            background-color: #1a252f;
            transform: translateY(-2px);
            color: white;
        }

        /* Abstract */
        .abstract-box {
            background-color: var(--light-bg);
            padding: 30px;
            border-radius: 12px;
            margin: 30px 0;
            border-left: 5px solid var(--primary-color);
        }
        .abstract-title { font-weight: bold; text-transform: uppercase; letter-spacing: 1px; margin-bottom: 10px; display: block; color: var(--primary-color); }

        /* Images */
        .figure {
            width: 100%;
            margin: 30px 0;
            text-align: center;
        }
        .figure img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 5px 15px rgba(0,0,0,0.05);
            margin-bottom: 10px;
        }
        .caption {
            font-size: 0.9rem;
            color: #666;
            margin-top: 8px;
            text-align: justify;
        }

        /* Tables */
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            font-size: 0.9rem;
        }
        th, td {
            padding: 12px;
            text-align: center;
            border-bottom: 1px solid #ddd;
        }
        th { background-color: var(--light-bg); font-weight: 600; }
        tr:hover { background-color: #f1f1f1; }
        .highlight-row { background-color: #e8f6f3; font-weight: bold; }

        /* Code Block */
        pre {
            background-color: #f4f4f4;
            padding: 20px;
            border-radius: 8px;
            overflow-x: auto;
            font-size: 0.85rem;
            line-height: 1.4;
        }

        /* Responsive */
        @media (max-width: 600px) {
            h1 { font-size: 1.8rem; }
            .btn { width: 100%; justify-content: center; }
        }
    </style>
</head>
<body>

<div class="container">
    
    <header class="header">
        <h1>Are We Using the Right Benchmark?<br>An Evaluation Framework for Visual Token Compression Methods</h1>
        
        <div class="authors">
            <span class="author-block">Chenfei Liao<sup>1,2,6</sup></span>
            <span class="author-block">Wensong Wang<sup>3,2</sup></span>
            <span class="author-block">Zichen Wen<sup>2,5</sup></span>
            <span class="author-block">Xu Zheng<sup>1,4,6</sup></span>
            <span class="author-block">Yiyu Wang<sup>2</sup></span>
            <br>
            <span class="author-block">Haocong He<sup>2</sup></span>
            <span class="author-block">Yuanhuiyi Lyu<sup>1,6</sup></span>
            <span class="author-block">Lutao Jiang<sup>1,6</sup></span>
            <span class="author-block">Xin Zou<sup>1,6</sup></span>
            <span class="author-block">Yuqian Fu<sup>4</sup></span>
            <span class="author-block">Bin Ren<sup>7,8,4</sup></span>
            <br>
            <span class="author-block">Linfeng Zhang<sup>†</sup></span>
            <span class="author-block">Xuming Hu<sup>1,6,†</sup></span>
        </div>

        <div class="affiliations">
            <sup>1</sup>HKUST (Guangzhou) &nbsp;
            <sup>2</sup>Shanghai AI Laboratory &nbsp;
            <sup>3</sup>Northeastern University &nbsp;
            <sup>4</sup>HKUST<br>
            <sup>5</sup>Shanghai Jiao Tong University &nbsp;
            <sup>6</sup>INSAIT &nbsp;
            <sup>7</sup>University of Pisa &nbsp;
            <sup>8</sup>University of Trento
        </div>

        <div class="notes">
            <sup>†</sup> Corresponding Authors
        </div>

        <div class="link-buttons">
            <a href="https://arxiv.org/abs/2510.07143" class="btn" target="_blank"><i class="fas fa-file-pdf"></i> Paper (arXiv)</a>
            <a href="https://github.com/Chenfei-Liao/VTC-Bench" class="btn" target="_blank"><i class="fab fa-github"></i> Code & Data</a>
        </div>
    </header>

    <div class="figure">
        <img src="https://via.placeholder.com/850x300?text=Figure+1:+Average+Decline+Ratio+Comparison+(Downsampling+vs+SOTA)" alt="Teaser: Downsampling outperforms SOTA">
        <div class="caption">
            <strong>Figure 1: The Surprising Observation.</strong> (a) Average Decline Ratio (ADR) of five visual token compression methods on eight benchmarks. Strikingly, simple <strong>Downsample</strong> (grey line) outperforms many advanced methods like VisionZip and FastV. (b) [cite_start]Despite its simplicity, Downsample achieves significantly faster inference times[cite: 35, 36].
        </div>
    </div>

    <div class="abstract-box">
        <span class="abstract-title">Abstract</span>
        <p>
            Recent endeavors to accelerate inference in Multimodal Large Language Models (MLLMs) have primarily focused on visual token compression. [cite_start]However, current benchmarks are originally designed to assess perception and reasoning, rather than compression techniques[cite: 9, 11].
        </p>
        <p>
            [cite_start]In this work, we reveal a surprising finding: <strong>simple image downsampling consistently outperforms many advanced compression methods</strong> across multiple widely used benchmarks[cite: 13]. This suggests that current benchmarks are "noisy" for this task—they contain too many simple samples that do not require fine-grained visual details.
        </p>
        <p>
            [cite_start]Motivated by this, we introduce <strong>VTC-Bench</strong>, an evaluation framework that uses downsampling as a data filter to distinguish between "simple" and "difficult" samples, enabling a fairer and more accurate assessment of visual token compression methods[cite: 16].
        </p>
    </div>

    <section>
        <h2>1. The Problem: Simplicity Bias in Benchmarks</h2>
        <p>
            Most MLLMs (like Qwen2-VL) natively support varying resolution inputs. [cite_start]We compared sophisticated token compression methods (FastV, VisionZip, PruMerge+, DART) against a naive baseline: simply resizing the image to a lower resolution (Downsample)[cite: 78, 92].
        </p>
        <p>
            The results were counterintuitive. As compression ratios increased (e.g., 99%), advanced methods collapsed, while Downsampling degraded gracefully. [cite_start]For instance, at 93.75% compression on MMBench, Downsampling achieved <strong>66.4%</strong> accuracy, outperforming the best advanced method (DART) by a significant margin[cite: 99].
        </p>
        <p>
            <strong>Hypothesis:</strong> Existing benchmarks are dominated by samples that only require low-resolution global information. [cite_start]We define these as "Simple Samples"[cite: 141, 142].
        </p>
    </section>

    <section>
        <h2>2. The Solution: VTC-Bench Framework</h2>
        <p>
            To denoise the evaluation, we propose <strong>VTC-Bench</strong>. It does not create new data but filters existing benchmarks (like GQA, MMBench, ChartQA) using downsampling as a discriminator.
        </p>

        <div class="figure">
            <img src="https://via.placeholder.com/850x350?text=Figure+3:+VTC-Bench+Pipeline+(Filter+Logic)" alt="VTC-Bench Pipeline">
            <div class="caption">
                <strong>Figure 2: The VTC-Bench Pipeline.</strong> We categorize samples into two groups. <strong>Group B (Simple):</strong> Samples correctly answered by the downsampling method. <strong>Group A (Difficult):</strong> Samples answered incorrectly by downsampling. [cite_start]We evaluate compression methods specifically on Group A to assess their ability to preserve critical details[cite: 168, 210].
            </div>
        </div>

        <h3>The Logic</h3>
        <p>
            For a target compression ratio \( C \), we apply an equivalent downsampling ratio \( D \) such that:
        </p>
        <p style="text-align: center; font-size: 1.2rem; margin: 20px 0;">
            $$ \frac{1}{D^2} \times 100\% = 1 - C $$
        </p>
        <p>
            We then filter out samples where the downsampling baseline is already correct (100% accuracy definition in Group B). [cite_start]This leaves us with the samples that genuinely require the capabilities of advanced compression algorithms[cite: 95, 153].
        </p>
    </section>

    <section>
        <h2>3. Key Results & Insights</h2>
        <p>
            When evaluated using VTC-Bench (focusing on "Difficult Samples"), the landscape of visual token compression changes completely. Advanced methods prove their worth where it matters most.
        </p>
        
        [cite_start]<p><strong>Table: Performance on "Difficult Samples" (Group A) at 75% Compression (Qwen2-VL-7B)</strong> [cite: 136]</p>
        <table>
            <thead>
                <tr>
                    <th>Method</th>
                    <th>ChartQA</th>
                    <th>OCRBench</th>
                    <th>GQA</th>
                    <th>MME</th>
                </tr>
            </thead>
            <tbody>
                <tr style="color: #999; font-style: italic;">
                    <td>Downsample (Baseline)</td>
                    <td>0.0%</td>
                    <td>0.0%</td>
                    <td>0.0%</td>
                    <td>0.0%</td>
                </tr>
                <tr>
                    <td>FastV</td>
                    <td>35.0%</td>
                    <td>29.1%</td>
                    <td>57.8%</td>
                    <td>65.4%</td>
                </tr>
                <tr class="highlight-row">
                    <td>VisionZip</td>
                    <td><strong>51.2%</strong></td>
                    <td>29.6%</td>
                    <td><strong>59.3%</strong></td>
                    <td>54.9%</td>
                </tr>
                <tr>
                    <td>DART</td>
                    <td>39.0%</td>
                    <td><strong>40.2%</strong></td>
                    <td>58.9%</td>
                    <td><strong>69.4%</strong></td>
                </tr>
            </tbody>
        </table>

        <h3>Major Takeaways</h3>
        <ul>
            [cite_start]<li><strong>Reversing the Trend:</strong> While Downsampling wins on the full noisy benchmark, advanced methods significantly outperform the baseline on the filtered VTC-Bench[cite: 225].</li>
            [cite_start]<li><strong>Task Sensitivity:</strong> Tasks requiring fine-grained visual understanding (like charts and OCR) show the largest gaps between sophisticated compression and naive downsampling[cite: 246].</li>
            [cite_start]<li><strong>Fairer Comparison:</strong> VTC-Bench amplifies the differences between methods, allowing researchers to see which algorithms truly retain critical visual context[cite: 245].</li>
        </ul>
    </section>

    <section>
        <h2>Citation</h2>
        <p>If you find VTC-Bench useful for your research, please cite our paper:</p>
        <pre><code>@article{liao2025are,
  title={Are We Using the Right Benchmark: An Evaluation Framework for Visual Token Compression Methods},
  author={Liao, Chenfei and Wang, Wensong and Wen, Zichen and Zheng, Xu and Wang, Yiyu and He, Haocong and Lyu, Yuanhuiyi and Jiang, Lutao and Zou, Xin and Fu, Yuqian and Ren, Bin and Zhang, Linfeng and Hu, Xuming},
  journal={arXiv preprint arXiv:2510.07143},
  year={2025}
}</code></pre>
    </section>

    <footer style="text-align: center; margin-top: 50px; font-size: 0.8rem; color: #888;">
        <p>&copy; 2025 VTC-Bench Project. Based on arXiv:2510.07143.</p>
    </footer>

</div>

</body>
</html>
