<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>VTC-Bench: Are We Using the Right Benchmark?</title>
    
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600&family=Playfair+Display:wght@700&display=swap" rel="stylesheet">
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <style>
        :root {
            --primary-color: #2c3e50;
            --accent-color: #3498db;
            --bg-color: #ffffff;
            --text-color: #4a4a4a;
            --light-bg: #f8f9fa;
        }

        body {
            font-family: 'Inter', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            background-color: var(--bg-color);
        }

        h1, h2, h3 {
            font-family: 'Playfair Display', serif;
            color: var(--primary-color);
            margin-bottom: 0.5em;
        }

        h1 { font-size: 2.5rem; line-height: 1.2; text-align: center; margin-bottom: 20px; }
        h2 { font-size: 1.8rem; border-bottom: 2px solid var(--light-bg); padding-bottom: 10px; margin-top: 50px; }
        h3 { font-size: 1.3rem; margin-top: 25px; }

        a { color: var(--accent-color); text-decoration: none; transition: 0.3s; }
        a:hover { color: #1d6fa5; }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 40px 20px;
        }

        /* Header Section */
        .header { text-align: center; margin-bottom: 50px; }
        
        .authors { font-size: 1.1rem; margin-bottom: 10px; }
        .author-block { display: inline-block; margin-right: 15px; }
        .affiliations { font-size: 0.9rem; color: #7f8c8d; margin-top: 10px; margin-bottom: 20px; }
        .notes { font-size: 0.85rem; color: #95a5a6; font-style: italic; }

        /* Buttons */
        .link-buttons {
            display: flex;
            justify-content: center;
            gap: 15px;
            margin-top: 20px;
            flex-wrap: wrap;
        }
        .btn {
            background-color: var(--primary-color);
            color: white;
            padding: 10px 20px;
            border-radius: 30px;
            font-weight: 600;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }
        .btn:hover {
            background-color: #1a252f;
            transform: translateY(-2px);
            color: white;
        }

        /* Abstract */
        .abstract-box {
            background-color: var(--light-bg);
            padding: 30px;
            border-radius: 12px;
            margin: 30px 0;
            border-left: 5px solid var(--primary-color);
        }
        .abstract-title { font-weight: bold; text-transform: uppercase; letter-spacing: 1px; margin-bottom: 10px; display: block; color: var(--primary-color); }

        /* Images */
        .figure {
            width: 100%;
            margin: 30px 0;
            text-align: center;
        }
        .figure img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 5px 15px rgba(0,0,0,0.05);
            margin-bottom: 10px;
        }
        .caption {
            font-size: 0.9rem;
            color: #666;
            margin-top: 8px;
            text-align: justify;
        }

        /* Tables */
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            font-size: 0.9rem;
        }
        th, td {
            padding: 12px;
            text-align: center;
            border-bottom: 1px solid #ddd;
        }
        th { background-color: var(--light-bg); font-weight: 600; }
        tr:hover { background-color: #f1f1f1; }
        .highlight-row { background-color: #e8f6f3; font-weight: bold; }

        /* Code Block */
        pre {
            background-color: #f4f4f4;
            padding: 20px;
            border-radius: 8px;
            overflow-x: auto;
            font-size: 0.85rem;
            line-height: 1.4;
        }

        /* Responsive */
        @media (max-width: 600px) {
            h1 { font-size: 1.8rem; }
            .btn { width: 100%; justify-content: center; }
        }
    </style>
</head>
<body>

<div class="container">
    
    <header class="header">
        <h1>Are We Using the Right Benchmark?<br>An Evaluation Framework for Visual Token Compression Methods</h1>
        
        <div class="authors">
            <span class="author-block">Chenfei Liao<sup>1,2,6</sup></span>
            <span class="author-block">Wensong Wang<sup>3,2</sup></span>
            <span class="author-block">Zichen Wen<sup>2,5</sup></span>
            <span class="author-block">Xu Zheng<sup>1,4,6</sup></span>
            <span class="author-block">Yiyu Wang<sup>2</sup></span>
            <br>
            <span class="author-block">Haocong He<sup>2</sup></span>
            <span class="author-block">Yuanhuiyi Lyu<sup>1,6</sup></span>
            <span class="author-block">Lutao Jiang<sup>1,6</sup></span>
            <span class="author-block">Xin Zou<sup>1,6</sup></span>
            <span class="author-block">Yuqian Fu<sup>4</sup></span>
            <span class="author-block">Bin Ren<sup>7,8,4</sup></span>
            <br>
            <span class="author-block">Linfeng Zhang<sup>†</sup></span>
            <span class="author-block">Xuming Hu<sup>1,6,†</sup></span>
        </div>

        <div class="affiliations">
            <sup>1</sup>HKUST (Guangzhou) &nbsp;
            <sup>2</sup>Shanghai AI Laboratory &nbsp;
            <sup>3</sup>Northeastern University &nbsp;
            <sup>4</sup>HKUST<br>
            <sup>5</sup>Shanghai Jiao Tong University &nbsp;
            <sup>6</sup>INSAIT &nbsp;
            <sup>7</sup>University of Pisa &nbsp;
            <sup>8</sup>University of Trento
        </div>

        <div class="notes">
            <sup>†</sup> Corresponding Authors
        </div>

        <div class="link-buttons">
            <a href="https://arxiv.org/abs/2510.07143" class="btn" target="_blank"><i class="fas fa-file-pdf"></i> Paper (arXiv)</a>
            <a href="https://github.com/Chenfei-Liao/VTC-Bench" class="btn" target="_blank"><i class="fab fa-github"></i> Code & Data</a>
        </div>
    </header>

    <div class="figure">
        <object data="Figure1.pdf" type="application/pdf" width="100%" height="480px">
            <a href="Figure1.pdf">Figure 1 PDF</a>
        </object>
        <div class="caption">
            <strong>Figure 1.</strong> Simple downsampling can match or surpass advanced compression on existing benchmarks, revealing a simplicity bias.
        </div>
    </div>

    <div class="abstract-box">
        <span class="abstract-title">Abstract</span>
        <p>
            Multimodal LLMs rely on visual token compression to speed up inference. Yet mainstream benchmarks were built for perception/reasoning, not for testing fidelity under compression. We find that <strong>simple image downsampling</strong> often matches or exceeds state-of-the-art compression methods, implying many samples are insensitive to fine-grained visual details.
        </p>
        <p>
            We introduce <strong>VTC-Bench</strong>, a framework that uses downsampling as a discriminator to split data into <strong>Group B (Simple)</strong>—samples still correct after aggressive downsampling—and <strong>Group A (Difficult)</strong>—samples that fail after downsampling. Evaluating compression on Group A yields a difficulty-aware, fairer comparison.
        </p>
        <p>
            On Group A, advanced methods show clear gains over naive downsampling; on Group B the gains vanish. VTC-Bench thus exposes the true strengths of visual token compression methods.
        </p>
    </div>

    <section>
        <h2>1. Motivation: Simplicity Bias in Current Benchmarks</h2>
        <p>
            Many benchmarks can be solved with low-resolution cues, masking the benefits of sophisticated compression. Downsampling alone can retain accuracy while reducing tokens, exposing the simplicity bias.
        </p>
    </section>

    <section>
        <h2>2. VTC-Bench Framework</h2>
        <p>
            We filter existing benchmarks (e.g., GQA, MMBench, ChartQA, MME) by comparing original vs. downsampled performance:
        </p>

        <div class="figure">
            <object data="Figure2.pdf" type="application/pdf" width="100%" height="420px">
                <a href="Figure2.pdf">Figure 2 PDF</a>
            </object>
            <div class="caption">
                <strong>Figure 2.</strong> VTC-Bench pipeline: Group B (Simple) remains correct after downsampling; Group A (Difficult) fails after downsampling.
            </div>
        </div>

        <h3>Grouping Logic</h3>
        <ul>
            <li><strong>Group B (Simple):</strong> Origin correct & downsample correct.</li>
            <li><strong>Group A (Difficult):</strong> Origin correct & downsample wrong.</li>
        </ul>
        <p>We focus evaluation on Group A to measure detail preservation and robustness under compression.</p>
    </section>

    <section>
        <h2>3. Key Results & Insights</h2>
        <p>
            When evaluated using VTC-Bench (focusing on "Difficult Samples"), the landscape of visual token compression changes completely. Advanced methods prove their worth where it matters most.
        </p>
        
        [cite_start]<p><strong>Table: Performance on "Difficult Samples" (Group A) at 75% Compression (Qwen2-VL-7B)</strong> [cite: 136]</p>
        <table>
            <thead>
                <tr>
                    <th>Method</th>
                    <th>ChartQA</th>
                    <th>OCRBench</th>
                    <th>GQA</th>
                    <th>MME</th>
                </tr>
            </thead>
            <tbody>
                <tr style="color: #999; font-style: italic;">
                    <td>Downsample (Baseline)</td>
                    <td>0.0%</td>
                    <td>0.0%</td>
                    <td>0.0%</td>
                    <td>0.0%</td>
                </tr>
                <tr>
                    <td>FastV</td>
                    <td>35.0%</td>
                    <td>29.1%</td>
                    <td>57.8%</td>
                    <td>65.4%</td>
                </tr>
                <tr class="highlight-row">
                    <td>VisionZip</td>
                    <td><strong>51.2%</strong></td>
                    <td>29.6%</td>
                    <td><strong>59.3%</strong></td>
                    <td>54.9%</td>
                </tr>
                <tr>
                    <td>DART</td>
                    <td>39.0%</td>
                    <td><strong>40.2%</strong></td>
                    <td>58.9%</td>
                    <td><strong>69.4%</strong></td>
                </tr>
            </tbody>
        </table>

        <h3>Major Takeaways</h3>
        <ul>
            <li><strong>Reversing the Trend:</strong> On the full benchmark downsampling looks strong, but on the filtered VTC-Bench advanced methods clearly outperform it.</li>
            <li><strong>Task Sensitivity:</strong> Fine-grained tasks (e.g., charts, OCR) show the largest gaps between sophisticated compression and naive downsampling.</li>
            <li><strong>Fairer Comparison:</strong> VTC-Bench magnifies differences, revealing which methods truly retain critical visual context.</li>
        </ul>
    </section>

    <section>
        <h2>Citation</h2>
        <p>If you find VTC-Bench useful for your research, please cite our paper:</p>
        <pre><code>@article{liao2025are,
  title={Are We Using the Right Benchmark: An Evaluation Framework for Visual Token Compression Methods},
  author={Liao, Chenfei and Wang, Wensong and Wen, Zichen and Zheng, Xu and Wang, Yiyu and He, Haocong and Lyu, Yuanhuiyi and Jiang, Lutao and Zou, Xin and Fu, Yuqian and Ren, Bin and Zhang, Linfeng and Hu, Xuming},
  journal={arXiv preprint arXiv:2510.07143},
  year={2025}
}</code></pre>
    </section>

    <footer style="text-align: center; margin-top: 50px; font-size: 0.8rem; color: #888;">
        <p>&copy; 2025 VTC-Bench Project. Based on arXiv:2510.07143.</p>
    </footer>

</div>

</body>
</html>
