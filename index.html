<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>VTC-Bench: Are We Using the Right Benchmark?</title>
    
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600&family=Playfair+Display:wght@700&display=swap" rel="stylesheet">
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <style>
        :root {
            --primary-color: #2c3e50;
            --accent-color: #3498db;
            --bg-color: #ffffff;
            --text-color: #4a4a4a;
            --light-bg: #f8f9fa;
        }

        body {
            font-family: 'Inter', sans-serif;
            line-height: 1.6;
            color: var(--text-color);
            margin: 0;
            padding: 0;
            background-color: var(--bg-color);
        }

        h1, h2, h3 {
            font-family: 'Playfair Display', serif;
            color: var(--primary-color);
            margin-bottom: 0.5em;
        }

        h1 { font-size: 2.5rem; line-height: 1.2; text-align: center; margin-bottom: 20px; }
        h2 { font-size: 1.8rem; border-bottom: 2px solid var(--light-bg); padding-bottom: 10px; margin-top: 50px; }
        h3 { font-size: 1.3rem; margin-top: 25px; }

        a { color: var(--accent-color); text-decoration: none; transition: 0.3s; }
        a:hover { color: #1d6fa5; }

        .container {
            max-width: 900px;
            margin: 0 auto;
            padding: 40px 20px;
        }

        /* Header Section */
        .header { text-align: center; margin-bottom: 50px; }
        
        .authors { font-size: 1.1rem; margin-bottom: 10px; }
        .author-block { display: inline-block; margin-right: 15px; }
        .affiliations { font-size: 0.9rem; color: #7f8c8d; margin-top: 10px; margin-bottom: 20px; }
        .notes { font-size: 0.85rem; color: #95a5a6; font-style: italic; }

        /* Buttons */
        .link-buttons {
            display: flex;
            justify-content: center;
            gap: 15px;
            margin-top: 20px;
            flex-wrap: wrap;
        }
        .btn {
            background-color: var(--primary-color);
            color: white;
            padding: 10px 20px;
            border-radius: 30px;
            font-weight: 600;
            display: inline-flex;
            align-items: center;
            gap: 8px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }
        .btn:hover {
            background-color: #1a252f;
            transform: translateY(-2px);
            color: white;
        }

        /* Abstract */
        .abstract-box {
            background-color: var(--light-bg);
            padding: 30px;
            border-radius: 12px;
            margin: 30px 0;
            border-left: 5px solid var(--primary-color);
        }
        .abstract-title { font-weight: bold; text-transform: uppercase; letter-spacing: 1px; margin-bottom: 10px; display: block; color: var(--primary-color); }

        /* Images */
        .figure {
            width: 100%;
            margin: 30px 0;
            text-align: center;
        }
        .figure img {
            max-width: 70%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 5px 15px rgba(0,0,0,0.05);
            margin-bottom: 10px;
        }
        .figure img.wide {
            max-width: 85%;
        }
        .caption {
            font-size: 0.9rem;
            color: #666;
            margin-top: 8px;
            text-align: justify;
        }

        /* Tables */
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            font-size: 0.9rem;
        }
        th, td {
            padding: 12px;
            text-align: center;
            border-bottom: 1px solid #ddd;
        }
        th { background-color: var(--light-bg); font-weight: 600; }
        tr:hover { background-color: #f1f1f1; }
        .highlight-row { background-color: #e8f6f3; font-weight: bold; }

        /* Code Block */
        pre {
            background-color: #f4f4f4;
            padding: 20px;
            border-radius: 8px;
            overflow-x: auto;
            font-size: 0.85rem;
            line-height: 1.4;
        }

        /* Responsive */
        @media (max-width: 600px) {
            h1 { font-size: 1.8rem; }
            .btn { width: 100%; justify-content: center; }
        }
    </style>
</head>
<body>

<div class="container">
    
    <header class="header">
        <h1>Are We Using the Right Benchmark?<br>An Evaluation Framework for Visual Token Compression Methods</h1>
        
        <div class="authors">
            <span class="author-block">Chenfei Liao<sup>1,2,6</sup></span>
            <span class="author-block">Wensong Wang<sup>3,2</sup></span>
            <span class="author-block">Zichen Wen<sup>2,5</sup></span>
            <span class="author-block">Xu Zheng<sup>1,4,6</sup></span>
            <span class="author-block">Yiyu Wang<sup>2</sup></span>
            <br>
            <span class="author-block">Haocong He<sup>2</sup></span>
            <span class="author-block">Yuanhuiyi Lyu<sup>1,6</sup></span>
            <span class="author-block">Lutao Jiang<sup>1,6</sup></span>
            <span class="author-block">Xin Zou<sup>1,6</sup></span>
            <span class="author-block">Yuqian Fu<sup>4</sup></span>
            <span class="author-block">Bin Ren<sup>7,8,4</sup></span>
            <br>
            <span class="author-block">Linfeng Zhang<sup>†</sup></span>
            <span class="author-block">Xuming Hu<sup>1,6,†</sup></span>
        </div>

        <div class="affiliations">
            <sup>1</sup>HKUST (Guangzhou) &nbsp;
            <sup>2</sup>Shanghai AI Laboratory &nbsp;
            <sup>3</sup>Northeastern University &nbsp;
            <sup>4</sup>HKUST<br>
            <sup>5</sup>Shanghai Jiao Tong University &nbsp;
            <sup>6</sup>INSAIT &nbsp;
            <sup>7</sup>University of Pisa &nbsp;
            <sup>8</sup>University of Trento
        </div>

        <div class="notes">
            <sup>†</sup> Corresponding Authors
        </div>

        <div class="link-buttons">
            <a href="https://arxiv.org/abs/2510.07143" class="btn" target="_blank"><i class="fas fa-file-pdf"></i> Paper (arXiv)</a>
            <a href="https://github.com/Chenfei-Liao/VTC-Bench" class="btn" target="_blank"><i class="fab fa-github"></i> Code & Data</a>
        </div>
    </header>

    <div class="abstract-box">
        <span class="abstract-title">Abstract</span>
        <p>
            Recent endeavors to accelerate inference in Multimodal Large Language Models (MLLMs) have primarily focused on visual token compression. The effectiveness of these methods is typically assessed by measuring the accuracy drop on established benchmarks, comparing model performance before and after compression.
        </p>
        <p>
            However, these benchmarks are originally designed to assess the perception and reasoning capabilities of MLLMs, rather than to evaluate compression techniques. As a result, directly applying them to visual token compression introduces a task mismatch. Strikingly, our investigation reveals that simple image downsampling consistently outperforms many advanced compression methods across multiple widely used benchmarks.
        </p>
        <p>
            Through extensive experiments, we make the following observations: (i) Current benchmarks are noisy for the visual token compression task. (ii) Down-sampling is able to serve as a data filter to evaluate the difficulty of samples in the visual token compression task. Motivated by these findings, we introduce VTC-Bench, an evaluation framework that incorporates a data filtering mechanism to denoise existing benchmarks, thereby enabling fairer and more accurate assessment of visual token compression methods. The code will be released to facilitate future research.
        </p>
    </div>

    <section>
        <h2>1. Motivation: Simplicity Bias in Current Benchmarks</h2>
        <p>
            Some data in the existing benchmarks is overly simplistic, leading to the unreasonable phenomenon that even the simplest downsampling method is sufficient to deal with the visual token compression task. This produces the counter-intuitive result that downsampling can outperform sophisticated compression methods, revealing that the benchmarks mix simple and difficult samples.
        </p>
        <div class="figure">
            <img src="Figure2.svg" alt="Average Decline Ratio and inference time">
            <div class="caption">
                <strong>Figure 2.</strong> (a) Average Decline Ratio (ADR) of five visual token compression methods on eight benchmarks (Model: Qwen2-VL-7B; Benchmark: as shown in Table 1; Device: 1 A800). (b) Inference time per image comparison of DART and Downsample (Model: Qwen2-VL-7B; Benchmark: MMStar; Compression Ratio: 0.75; Device: 1 A800).
            </div>
        </div>
    </section>

    <section>
        <h2>2. VTC-Bench Framework</h2>
        <p>
            We filter existing benchmarks (e.g., GQA, MMBench, ChartQA, MME) by comparing original vs. downsampled performance:
        </p>

        <div class="figure">
            <img class="wide" src="Figure1.svg" alt="VTC-Bench framework">
            <div class="caption">
                <strong>Figure 1.</strong> The VTC-Bench is a simple but effective framework that can transform any existing benchmarks to a subset that can fairly evaluate VTC (Visual Token Compression) methods. The samples that are answered correctly by the original Qwen2-VL model without downsampling form the input samples.
            </div>
        </div>

        <h3>Grouping Logic</h3>
        <ul>
            <li><strong>Group B (Simple):</strong> Samples that are answered correctly by the downsampling method.</li>
            <li><strong>Group A (Difficult):</strong> Samples that are answered incorrectly by the downsampling method.</li>
        </ul>
        <p>We then evaluate all compression methods on these two groups separately to assess whether the sophisticated methods demonstrate their expected superiority on the “difficult” samples where image downsampling fails.</p>
    </section>

    <section>
        <h2>3. Results & Insights</h2>
        <p>
            Across many benchmarks, simple image downsampling often beats more advanced compression methods, suggesting that sophisticated approaches are unnecessary. When we restrict evaluation to the compression-relevant difficult samples (Group A), the trend reverses: the apparent superiority of downsampling largely stems from original benchmarks being saturated with easy cases that do not require fine-grained cues.
        </p>
        <p>
            VTC-Bench amplifies and clarifies method differences. For example, the gaps between methods widen on the filtered difficult samples, indicating that VTC-Bench effectively removes data noise unrelated to visual token compression and promotes fair comparison.
        </p>

        <div class="figure">
            <table>
                <caption><strong>Table: Performance on Group A (Difficult) at 75% compression (Qwen2-VL-7B)</strong></caption>
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>ChartQA</th>
                        <th>OCRBench</th>
                        <th>GQA</th>
                        <th>MME</th>
                    </tr>
                </thead>
                <tbody>
                    <tr class="highlight-row">
                        <td>VisionZip</td>
                        <td>51.2%</td>
                        <td>29.6%</td>
                        <td>59.3%</td>
                        <td>54.9%</td>
                    </tr>
                    <tr>
                        <td>DART</td>
                        <td>39.0%</td>
                        <td>40.2%</td>
                        <td>58.9%</td>
                        <td>69.4%</td>
                    </tr>
                    <tr>
                        <td>FastV</td>
                        <td>35.0%</td>
                        <td>29.1%</td>
                        <td>57.8%</td>
                        <td>65.4%</td>
                    </tr>
                    <tr>
                        <td>Downsample</td>
                        <td>0.0%</td>
                        <td>0.0%</td>
                        <td>0.0%</td>
                        <td>0.0%</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="figure">
            <img src="Figure4.svg" alt="Group A vs Group B comparison at 75% compression">
            <div class="caption">
                <strong>Figure 4.</strong> Comparison of advanced token compression methods and downsampling on Qwen2-VL-7B by groups at 75% compression.
            </div>
        </div>
        <div class="figure">
            <img src="Figure3.svg" alt="VTC-Bench results on Qwen2-VL-7B">
            <div class="caption">
                <strong>Figure 3.</strong> VTC-Bench results on Qwen2-VL-7B.
            </div>
        </div>
        <h3>Major Takeaways</h3>
        <p>
            Across all benchmarks and compression methods, the accuracy on “simple” samples (Group B) is dramatically higher than on “difficult” samples (Group A); the two groups represent essentially different levels of visual comprehension difficulty. The 0%/100% dichotomy created by image downsampling provides ideal reference points. In Group B, advanced methods show comparable but not superior performance; in Group A, they significantly exceed the downsampling baseline (e.g., VisionZip 51.2% on ChartQA and DART 40.2% on OCRBench at 75% compression), proving their ability to preserve crucial details. Downsampling thus serves as a filter to denoise benchmarks for visual token compression. Moreover, VTC-Bench amplifies gaps: at 75% compression on ChartQA, the VisionZip–FastV gap widens from 8.8% to 16.2%; at 96% compression on GQA, it grows from 0.3% to 9.0%.
        </p>
    </section>

    <section>
        <h2>Citation</h2>
        <p>If you find VTC-Bench useful for your research, please cite our paper:</p>
        <pre><code>@article{liao2025are,
  title={Are We Using the Right Benchmark: An Evaluation Framework for Visual Token Compression Methods},
  author={Liao, Chenfei and Wang, Wensong and Wen, Zichen and Zheng, Xu and Wang, Yiyu and He, Haocong and Lyu, Yuanhuiyi and Jiang, Lutao and Zou, Xin and Fu, Yuqian and Ren, Bin and Zhang, Linfeng and Hu, Xuming},
  journal={arXiv preprint arXiv:2510.07143},
  year={2025}
}</code></pre>
    </section>

    <footer style="text-align: center; margin-top: 50px; font-size: 0.8rem; color: #888;">
        <p>&copy; 2025 VTC-Bench Project. Based on arXiv:2510.07143.</p>
    </footer>

</div>

</body>
</html>
